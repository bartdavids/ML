# -*- coding: utf-8 -*-
"""
Created by: bartdavids: https://github.com/bartdavids/ML/

"""
import numpy as np

class NN():
    """
    TODO: 
        Batch training
        Set history to a type of loss function or something
    """
    
    def __init__(self, i, labels, epochs, hidden = [], learning_rate = 0.01, beta1 = 0.8, beta2 = 0.999, eps = 1e-8, optimizer = 'gradient descent', activation = 'sigmoid', loss = 'mean squared error'):
        
        # Some assignments to the NN object
        self.i = i
        self.labels = labels
        self.epochs = epochs
        self.learning_rate = learning_rate
        
        # self.epoch will show the current epoch of the training,
        # self.history the evolution of the evaluationg function 
        # (only % of correct answers for now)
        self.epoch = 0
        self.history = []
        
        # neurons = n, where each entry is a layer, which value represents 
        # the amount of neurons in that layer
        self.n = [len(i[0])] + hidden + [len(labels[0])]
        
        # initialize weights (w) between -0.5 and 0.5 and biases (b) as 0
        self.w = [np.random.uniform(-0.5, 0.5, (self.n[h+1], self.n[h])) for h in range(len(self.n)-1)]
        self.b = [np.zeros((self.n[h + 1], 1)) for h in range(len(self.n)-1)] 

        # set optimizer
        if optimizer.lower() == 'gradient descent' or optimizer.lower() == 'gd':
            self.optimizer = self.gradient_descent
        elif optimizer.lower() == 'adam':
            self.optimizer = self.adam
            
            # Initialize momentii m1 and m2 for weights, 
            # mb1, mb2 for the biases
            self.m1 = [np.zeros(x.shape) for x in self.w]
            self.m2 = [np.zeros(x.shape) for x in self.w]
            self.bm1 = [np.zeros(x.shape) for x in self.b]
            self.bm2 = [np.zeros(x.shape) for x in self.b]
            
            # set additional hyperparameters
            self.beta1 = beta1
            self.beta2 = beta2
            self.eps = eps
            
        else:
            raise(Exception(f'The optimizer {optimizer} is not available'))
        
        # set activation functions, and its derivative
        if activation.lower() == 'sigmoid':
            self.activation = self.sigmoid
            self.derivitive_activation = self.der_sigmoid
        elif activation.lower() == 'tanh':
            self.activation = self.tanh
            self.derivitive_activation = self.der_tanh
        elif activation.lower() == 'relu':
            self.activation = self.relu
            self.derivitive_activation = self.der_relu
        else:
            raise(Exception(f'The activation function {activation} is not available'))
            
        # set loss function and its derivative (der_loss)
        if loss.lower() == 'mean squared error' or loss.lower() == 'mse':
            self.loss = self.mse
            self.der_loss = self.der_mse            
        elif loss.lower() == 'mean absolute error' or loss.lower() == 'mae':
            self.loss = self.mae
            self.der_loss = self.der_mae
        else:
            raise(Exception(f'The loss function {loss} is not available'))
    
    def set_learning_rate(self, learning_rate):
        # function to adjust learning rate after training to continue
        self.learning_rate = learning_rate
        
    def sigmoid(self, r_pre):
        # sigmoid activation function
        return 1 / (1 + np.exp(-r_pre))
    
    def relu(self, r_pre):
        # relu activation function
        # causes matmul overflow when used
        return np.max((np.zeros(r_pre.shape), r_pre), axis = 0)
    
    def der_sigmoid(self, r_der):
        # sigmoid activation function derivative for backprop
        return (r_der * (1 - r_der))
    
    def der_relu(self, r):
        # relu activation function derivative for backprop
        der_r = r.copy()
        der_r[der_r<=0] = 0
        der_r[der_r>0] = 1
        return der_r
    
    def tanh(self, r):
        return ((np.exp(r) - np.exp(-r))/(np.exp(r) + np.exp(-r)))
    
    def der_tanh(self, r):
        return 1 - ((np.exp(r) - np.exp(-r))**2) / ((np.exp(r) - np.exp(-r))**2)
    
    # Cost functions and their derivatives
    def mse(self, y, r):
        # mean squared error loss function
        return 1 / (2 * len(r)) * np.sum((y - r) ** 2, axis=0)
    
    def der_mse(self, y, r):
        # derivative of the mean squared error function for backprop
        return 1*(y - r)
        
    def mae(self, y, r):
        # mean absolute error loss function
        return(abs(y-r))
    
    def der_mae(self, y, r):
        # derivative of the mean absolute error function for backprop
        e = y-r
        e[e<0] = -1
        e[e>0] = 1
        return e
    
    # Backpropogation algorithms
    def gradient_descent(self, r, l, w, b):
        """
        Backpropogation through the NN neural network to adjust the weights according to residuals
        using the method gradient descent
        
        Parameters
        ----------
        r : list of nx1 nupy arrays
            The results after activation. The first in the list is the input value, the last is the ouput value
        l : nx1 nupmy array
            contains the ouput according to the training set (labels).
        w : list of nxm numpy arrays
            The weights.
        b : list of nx1 numpy arrays
            The biases.

        Returns
        -------
        w : list of nxm numpy arrays
            The adjusted weights.
        b : list of nxm numpy arrays
            The adjusted biases.
        """
        
        grad = list(range(len(w))) # list of gradients for eacht activated neuron (so all except input)
        
        # occurs in two stages: from output to hidden and then the rest
        # Backpropagation output -> hidden (cost function derivative):
        grad_o = self.der_loss(r[-1], l) # derrivative of the cost function for gradiënt descent
        grad[-1] = grad_o
        
        # adjust weights and biases between output and final hidden layer
        w[-1] += -self.learning_rate * grad_o @ r[-2].T
        b[-1] += -self.learning_rate * grad_o
        
        # Backpropagation hidden layers (activation function derivative)
        for hli in list(range(len(w)-1))[::-1]: # for all the connections beteen the hidden layers, in reverse
            
            grad_h = w[hli + 1].T @ grad[hli + 1] * self.derivitive_activation(r[hli + 1])
            grad[hli] = grad_h
            
            # update weights
            w[hli] += -self.learning_rate * grad_h @ r[hli].T
            b[hli] += -self.learning_rate * grad_h
        return w, b
    
    def adam(self, r, l, w, b):
        
        """
        Backpropogation through the NN neural network to adjust the weights according to residuals
        using the adam method:
        https://arxiv.org/pdf/1412.6980.pdf
        
        Parameters
        ----------
        r : list of nx1 nupy arrays
            The results after activation. The first in the list is the input value, the last is the ouput value
        l : nx1 nupmy array
            contains the ouput according to the training set (labels).
        w : list of nxm numpy arrays
            The weights.
        b : list of nx1 numpy arrays
            The biases.

        Returns
        -------
        w : list of nxm numpy arrays
            The adjusted weights.
        b : list of nxm numpy arrays
            The adjusted biases.
        """
        
        # Backpropagation output -> hidden (cost function derivative)
        grad = list(range(len(w))) # list of gradients for eacht activated neuron (so all except input)
        
        grad_o = self.der_loss(r[-1], l) # derrivative of the cost function for gradiënt descent
        grad[-1] = grad_o
        
        # update momentum
        self.m1[-1] = self.beta1 * self.m1[-1] + (1.0 - self.beta1) * grad_o @ r[-2].T
        self.m2[-1] = self.beta2 * self.m2[-1] + (1.0 - self.beta2) * (grad_o**2 @ r[-2].T)
        self.bm1[-1] = self.beta1 * self.bm1[-1] + (1.0 - self.beta1) * grad_o
        self.bm2[-1] = self.beta2 * self.bm2[-1] + (1.0 - self.beta2) * grad_o**2
        
        # Bias correction
        m1hat = self.m1[-1] / (1. - self.beta1**(self.epoch + 1))
        m2hat = self.m2[-1] / (1. - self.beta2**(self.epoch + 1))
        bm1hat = self.bm1[-1] / (1. - self.beta1**(self.epoch + 1))
        bm2hat = self.bm2[-1] / (1. - self.beta2**(self.epoch + 1))
        
        # Update variable value            
        w[-1] += -self.learning_rate * m1hat/(np.sqrt(m2hat) + self.eps) 
        b[-1] += -self.learning_rate * bm1hat/(np.sqrt(bm2hat) + self.eps)
        
         # Backpropagation hidden layers (activation function derivative)
        for hli in list(range(len(w)-1))[::-1]: # for all the connections beteen the hidden layers, in reverse
            
            grad_h = w[hli + 1].T @ grad[hli + 1] * self.derivitive_activation(r[hli + 1])
            grad[hli] = grad_h
            
            # update momentum
            self.m1[hli] = self.beta1 * self.m1[hli] + (1.0 - self.beta1) * grad_h @ r[hli].T
            self.m2[hli] = self.beta2 * self.m2[hli] + (1.0 - self.beta2) * (grad_h**2 @ r[hli].T)
            self.bm1[hli] = self.beta1 * self.bm1[hli] + (1.0 - self.beta1) * grad_h 
            self.bm2[hli] = self.beta2 * self.bm2[hli] + (1.0 - self.beta2) * grad_h**2
            
            # Bias correction
            m1hat = self.m1[hli] / (1. - self.beta1**(self.epoch + 1))
            m2hat = self.m2[hli] / (1. - self.beta2**(self.epoch + 1))
            bm1hat = self.bm1[hli] / (1. - self.beta1**(self.epoch + 1))
            bm2hat = self.bm2[hli] / (1. - self.beta2**(self.epoch + 1))
            
            #update weights
            w[hli] += -self.learning_rate * m1hat/(np.sqrt(m2hat) + self.eps)
            b[hli] += -self.learning_rate * bm1hat/(np.sqrt(bm2hat) + self.eps)
        return w, b
    
    def Train(self):
        """
        Train the model

        Returns
        -------
        None, it adjusts the weights of the NN object

        """
        for _epoch in range(self.epochs):
            correct = 0
            for img, l in zip(self.i, self.labels):
                
                r = [] # The results are stored in a list of nx1 matrices for each layer
                img.shape += (1,) # make sure the numpy array (vector) is a matrix (from vector(10) to matraix (10,1))        
                l.shape += (1,)
                r.append(img)
                
                for lay in range(len(self.w)): 
                    # Forward propagation input -> hidden = step 0.
                    # Forward propagation hiden -> ouput = step -1.
                    r_pre = self.b[lay] + self.w[lay] @ r[lay]  # pre = pre-activation
                    r.append(self.activation(r_pre)) # activation (normalisation with the sigmoid function), between hidden and input
        
                correct += int(np.argmax(r[-1]) == np.argmax(l)) #The highest number is chosen, "correct" is only to determine the amount of correct outcomes per epoch
                self.w, self.b = self.optimizer(r, l, self.w, self.b)   
        
            # Show accuracy for this epoch
            # percentage correct:
            cor = round((correct / self.i.shape[0]) * 100, 2)
            self.history.append(cor)
            print(f"Epoch {_epoch+1}: Accuracy = {cor}%")
            self.epoch += 1
    
    def Predict(self, i):
        i.shape += (1,) # make sure the numpy array (vector) is a matrix (from vector(10) to matraix (10,1))
        r = [i]
        for lay in range(len(self.w)): 
            # Forward propagation input -> hidden = step 0.
            # Forward propagation hiden -> ouput = step -1.
            r_pre = self.b[lay] + self.w[lay] @ r[lay]  # pre = pre-activation
            r.append(self.activation(r_pre))
        return r[-1]
        
